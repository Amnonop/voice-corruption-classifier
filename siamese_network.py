# -*- coding: utf-8 -*-
"""siamese_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qCBFwdgpGgdwlInk-Iz5EziXy1uu9C5O
"""

# Commented out IPython magic to ensure Python compatibility.
from pathlib import Path
import uuid
import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
from torch.optim import Adam
from torch.nn.functional import pairwise_distance
import torch
import torch.nn as nn
from torch.nn import Module
import torch.nn.functional as F

from torchsummary import summary

import json
from pathlib import Path

# %matplotlib inline

ROOT_DIR = './data'
DATA_FILENAME_PREFIX = 'siamese_{0}.pickle'
RESULTS_DIR = 'siamese'
WEIGHTS_PATH_FORMAT = 'siamese_weights_epoch_{0:.2f}_acc_{1:.2f}.pt'

SAMPLE_SIZE = 32000

SEED = 1

class Siamese(nn.Module):
  def __init__(self):
    super(Siamese, self).__init__()

    self.conv_block1 = nn.Sequential(
        nn.Conv1d(in_channels=1, out_channels=64, kernel_size=80, stride=4),
        nn.BatchNorm1d(num_features=64),
        nn.ReLU(),
        nn.MaxPool1d(kernel_size=4)
    )

    self.conv_block2 = nn.Sequential(
        nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=64),
        nn.ReLU(),
        nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=64),
        nn.ReLU(),
        nn.MaxPool1d(kernel_size=4)
    )

    self.conv_block3 = nn.Sequential(
        nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=128),
        nn.ReLU(),
        nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=128),
        nn.ReLU(),
        nn.MaxPool1d(kernel_size=4)
    )

    self.conv_block4 = nn.Sequential(
        nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=256),
        nn.ReLU(),
        nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=256),
        nn.ReLU(),
        nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=256),
        nn.ReLU(),
        nn.MaxPool1d(kernel_size=4)
    )

    self.conv_block5 = nn.Sequential(
        nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=512),
        nn.ReLU(),
        nn.Conv1d(in_channels=512, out_channels=512, kernel_size=3, stride=1),
        nn.BatchNorm1d(num_features=512),
        nn.ReLU()
    )

    self.avg_pool = nn.AvgPool1d(3)

    self.linear = nn.Linear(4096, 256)

    self.l1_layer = lambda tensors: torch.abs(tensors[0] - tensors[1])

    self.out = nn.Sequential(nn.Linear(256, 2), nn.Sigmoid())

  def forward_one(self, x):
    x = self.conv_block1(x)
    x = self.conv_block2(x)
    x = self.conv_block3(x)
    x = self.conv_block4(x)
    x = self.conv_block5(x)

    # Global avg pooling
    x = self.avg_pool(x)

    # Dense
    x = x.view(x.size(0), -1)
    x = self.linear(x)
    return x

  def forward(self, x1, x2):
    out1 = self.forward_one(x1)
    out2 = self.forward_one(x2)
    l1_distance = self.l1_layer([out1, out2])
    prediction = self.out(l1_distance)

    return prediction

import pickle
from pathlib import Path

import numpy as np
import torch
from torch import Tensor
import librosa
from sklearn.utils import shuffle

CSV_FILENAME_PREFIX = 'siamese_speakers_{0}.csv'

DATASET_FILE_FORMAT = 'siamese_{0}_{1}.npy'

class SiameseLoader:
    _data_subsets = ['train', 'test']

    def __init__(self, root_dir: str, device = None):
        self._data = {}
        self._categories = {}
        self._classes_map = {}
        self._device = device

        for name in self._data_subsets:
            filepath = Path(root_dir).joinpath(DATASET_FILE_FORMAT.format(name, SAMPLE_SIZE))
            print(f'Loading data from {filepath}')
            x = np.load(filepath)
            self._data[name] = x

    def get_batch(self, batch_size: int, mode: str = 'train'):
        x = self._data[mode]
        num_classes = x.shape[0] 
        num_examples = x.shape[1]

        # Randomly select sample classes for the batch
        categories = np.random.choice(num_classes, size=(batch_size, ), replace=False)

        # Initialize the pairs tensor
        pairs = [np.zeros(shape=(batch_size, 1, SAMPLE_SIZE)) for i in range(2)]

        # Initialize targets so half of samples are from same class
        # 1 - SAME CLASS (probability)
        # 0 - DIFFERENT CLASS
        targets = np.zeros(shape=(batch_size, ))
        targets[batch_size // 2:] = 1
        for i in range(batch_size):
            category = categories[i]
            first_index = np.random.randint(0, num_examples)
            pairs[0][i, :, :] = x[category, first_index].reshape(1, -1)

            second_index = np.random.randint(0, num_examples)
            if i >= batch_size // 2:
                second_category = category
            else:
                second_category = (category + np.random.randint(1, num_classes)) % num_classes

            pairs[1][i, :, :] = x[second_category, second_index].reshape(1, -1)

        pairs, targets = [torch.from_numpy(z).to(self._device) for z in [np.array(pairs).astype(np.float32), np.array(targets).astype(np.long)]]

        return pairs, targets

    def make_oneshot_task(self, N, mode='test'):
        # 1 - SAME CLASS (probability)
        # 0 - DIFFERENT CLASS
        x = self._data[mode]
        num_classes = x.shape[0] 
        num_examples = x.shape[1]
        indices = np.random.randint(0, num_examples, size=(N, ))
        categories = np.random.choice(range(num_classes), size=(N, ), replace=False)

        true_category = categories[0]
        first_example, second_example = np.random.choice(num_examples, replace=False, size=(2, ))

        test_sample = np.asarray([x[true_category, first_example]] * N).reshape(N, 1, -1)

        support_set = x[categories, indices]
        support_set[0, :] = x[true_category, second_example]
        support_set = support_set.reshape(N, 1, -1)

        targets = np.zeros(shape=(N, ))
        targets[0] = 1

        targets, test_sample, support_set = shuffle(targets, test_sample, support_set)
        pairs = [test_sample, support_set]

        pairs, targets = [torch.from_numpy(z).to(self._device) for z in [np.array(pairs).astype(np.float32), np.array(targets).astype(np.long)]]

        return pairs, targets

def test_oneshot(model, data_loader, N, k):
  number_correct = 0

  print(f'Evaluating model on {k} random {N} way one-shot learning tasks.')

  model.eval()
  accuracies = []
  with torch.no_grad():
      for i in range(k):
          inputs, targets = data_loader.make_oneshot_task(N)
          predictions = model(inputs[0], inputs[1])
          accuracies.append((predictions.argmax(dim=1) == targets).detach())
          
        #   if torch.argmax(predictions) == torch.argmax(targets):
        #       number_correct += 1

#   percent_correct = (100.0 * number_correct / k)
  # print(f'Got an average of {percent_correct}% {N} way one-shot learning accuracy')
  percent_correct = 100. * torch.cat(accuracies).float().mean().item()
  return percent_correct

def plot(log):
  df = pd.DataFrame(log)

  fig, ax = plt.subplots(figsize=(6, 4))
  # train_df = df[df['mode'] == 'train']
  test_df = df[df['mode'] == 'test']
  # ax.plot(train_df['epoch'], train_df['acc'], label='Train')
  ax.plot(test_df['epoch'], test_df['acc'], label='Test')
  ax.set_xlabel('Epoch')
  ax.set_ylabel('Accuracy')
  ax.set_ylim(0, 100)
  fig.legend(ncol=2, loc='lower right')
  fig.tight_layout()
  plt.title('Siamese Accuracy for Epoch')
  plt.show()

from pathlib import Path
import json

def get_epoch_from_file(filename: str) -> float:
  filename_parts = filename.split('_')
  epoch_index = filename_parts.index('epoch')
  if epoch_index == -1:
    return -1.0
  return float(filename_parts[epoch_index + 1])

def get_latest_weights_filename(root_dir: str = ROOT_DIR) -> str:
    weights_dir = Path(root_dir)

    # Find the weights file with the latest epoch number
    current_epoch = -1.0
    weights_filepath = None
    for filepath in weights_dir.iterdir():
        filename = filepath.name
        if filename.startswith('siamese_weights'):
            epoch = get_epoch_from_file(filename)
            if epoch > current_epoch:
                weights_filepath = filepath
                current_epoch = epoch

    return weights_filepath

seed = SEED
load_weights = False
start_epoch = 0

torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed(seed)
np.random.seed(seed)

# Set up the dataset loader
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

data_dir_path = Path(ROOT_DIR)

results_dir_path = Path(ROOT_DIR).joinpath(RESULTS_DIR)

# Load data
data_loader = SiameseLoader(data_dir_path, device)

model = Siamese().to(device)
if load_weights:
  weights_filename = get_latest_weights_filename()
  print(f'Loading weights from {weights_filename}')
  model.load_state_dict(torch.load(weights_filename))
  start_epoch = int(get_epoch_from_file(Path(weights_filename).name))
  print(f'Starting from epoch {start_epoch}')

criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=0.00008)

evaluate_every = 10  # interval for evaluating on one-shot tasks
loss_every = 20  # interval for printing loss (iterations)
batch_size = 32
num_iterations = 4000 

N_way = 5  # how many classes for testing one-shot tasks
n_val = 250  # how many one-shot tasks to validate on?
best = -1

state_filename = f'{uuid.uuid1()}_state_{num_iterations}_iters.pth'
state_path = results_dir_path.joinpath('best_snapshot').joinpath(state_filename)

loss_history = []
loss_iterations = []
log = []

print("Starting training process!")
print("-------------------------------------")
for i in range(start_epoch, num_iterations):
    start_time = time.time()
    (inputs, targets) = data_loader.get_batch(batch_size)
    
    # switch model to training mode
    model.train()

    # clear gradient accumulators
    optimizer.zero_grad()

    # forward pass
    predictions = model(inputs[0], inputs[1])

    # calculate loss of the network output with respect to the training labels
    loss = criterion(predictions, targets)

    # backpropagate and update optimizer learning rate
    loss.backward()
    optimizer.step()

    iterration_time = time.time() - start_time
    if i % evaluate_every == 0:
        val_acc = test_oneshot(model, data_loader, N_way, n_val)
        print(f'[Epoch {i:.2f}] Test Loss: 0.0 | Acc: {val_acc:.2f}')
        log.append({
            'epoch': i,
            'loss': 0.0,
            'acc': val_acc,
            'mode': 'test',
            'time': time.time(),
        })

        if val_acc >= best:
            print(f'Current best {val_acc:.2f}, previous best {best:.2f}')
            weights_path = results_dir_path.joinpath(WEIGHTS_PATH_FORMAT.format(i, val_acc))
            print(f'Saving weights to {weights_path}')
            torch.save(model.state_dict(), weights_path)
            best = val_acc

        # plot(log)

    if i % loss_every == 0:
        print(f'[Epoch {i:.2f}] Train Loss: {loss.item():.2f} | Acc: 0.0 | Time: {iterration_time:.2f}')
        log.append({
            'epoch': i,
            'loss': loss.item(),
            'acc': 0.0,
            'mode': 'train',
            'time': time.time(),
        })
        loss_iterations.append(i)
        loss_history.append(loss.item())

plt.figure()
plt.title(f'Model Loss for iterations')
plt.xlabel('iteration')
plt.ylabel('loss')
plt.plot(loss_history, label='train')
plt.legend()
plt.show()